{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `triton.jit`'ed functions can be auto-tuned by using the `triton.autotune` decorator, which consumes:\n",
    "#   - A list of `triton.Config` objects that define different configurations of\n",
    "#       meta-parameters (e.g., `BLOCK_SIZE_M`) and compilation options (e.g., `num_warps`) to try\n",
    "#   - An auto-tuning *key* whose change in values will trigger evaluation of all the\n",
    "#       provided configs\n",
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3,\n",
    "                      num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5,\n",
    "                      num_warps=2),\n",
    "        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5,\n",
    "                      num_warps=2),\n",
    "        # Good config for fp8 inputs.\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 128}, num_stages=3,\n",
    "                      num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 128}, num_stages=3,\n",
    "                      num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 128}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 128}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "    ],\n",
    "    key=['M', 'N', 'K'],\n",
    ")\n",
    "@triton.jit\n",
    "def naive_matmul_kernel(\n",
    "    a_ptr, b_ptr, c_ptr,\n",
    "    M, N, K,\n",
    "    stride_am, stride_ak,\n",
    "    stride_bk, stride_bn,\n",
    "    stride_cm, stride_cn,\n",
    "    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n",
    "):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    grid_n = (N + BLOCK_SIZE_N - 1) // BLOCK_SIZE_N\n",
    "\n",
    "    pid_m = pid // grid_n\n",
    "    pid_n = pid % grid_n\n",
    "\n",
    "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n",
    "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n",
    "\n",
    "    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "\n",
    "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
    "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n",
    "\n",
    "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n",
    "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
    "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "\n",
    "        accumulator = tl.dot(a, b, accumulator)\n",
    "\n",
    "        a_ptrs += BLOCK_SIZE_K * stride_ak\n",
    "        b_ptrs += BLOCK_SIZE_K * stride_bk\n",
    "    \n",
    "    c = accumulator.to(tl.float16)\n",
    "\n",
    "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "\n",
    "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n",
    "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n",
    "    tl.store(c_ptrs, c, mask=c_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `triton.jit`'ed functions can be auto-tuned by using the `triton.autotune` decorator, which consumes:\n",
    "#   - A list of `triton.Config` objects that define different configurations of\n",
    "#       meta-parameters (e.g., `BLOCK_SIZE_M`) and compilation options (e.g., `num_warps`) to try\n",
    "#   - An auto-tuning *key* whose change in values will trigger evaluation of all the\n",
    "#       provided configs\n",
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3,\n",
    "                      num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5,\n",
    "                      num_warps=2),\n",
    "        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5,\n",
    "                      num_warps=2),\n",
    "        # Good config for fp8 inputs.\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=3,\n",
    "                      num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=3,\n",
    "                      num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "    ],\n",
    "    key=['M', 'N', 'K'],\n",
    ")\n",
    "@triton.jit\n",
    "def matmul_kernel(\n",
    "        # Pointers to matrices\n",
    "        a_ptr, b_ptr, c_ptr,\n",
    "        # Matrix dimensions\n",
    "        M, N, K,\n",
    "        # The stride variables represent how much to increase the ptr by when moving by 1\n",
    "        # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n",
    "        # by to get the element one row down (A has M rows).\n",
    "        stride_am, stride_ak,  #\n",
    "        stride_bk, stride_bn,  #\n",
    "        stride_cm, stride_cn,\n",
    "        # Meta-parameters\n",
    "        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  #\n",
    "        GROUP_SIZE_M: tl.constexpr,  #\n",
    "):\n",
    "    \"\"\"Kernel for computing the matmul C = A x B.\n",
    "    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n",
    "    \"\"\"\n",
    "    # -----------------------------------------------------------\n",
    "    # Map program ids `pid` to the block of C it should compute.\n",
    "    # This is done in a grouped ordering to promote L2 data reuse.\n",
    "    # See above `L2 Cache Optimizations` section for details.\n",
    "    pid = tl.program_id(axis=0)\n",
    "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n",
    "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n",
    "    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_m = group_id * GROUP_SIZE_M\n",
    "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n",
    "    pid_m = first_pid_m + (pid % group_size_m)\n",
    "    pid_n = (pid % num_pid_in_group) // group_size_m\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Create pointers for the first blocks of A and B.\n",
    "    # We will advance this pointer as we move in the K direction\n",
    "    # and accumulate\n",
    "    # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\n",
    "    # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers\n",
    "    # See above `Pointer Arithmetic` section for details\n",
    "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n",
    "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n",
    "    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
    "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Iterate to compute a block of the C matrix.\n",
    "    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n",
    "    # of fp32 values for higher accuracy.\n",
    "    # `accumulator` will be converted back to fp16 after the loop.\n",
    "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n",
    "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
    "        # Load the next block of A and B, generate a mask by checking the K dimension.\n",
    "        # If it is out of bounds, set it to 0.\n",
    "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "        # We accumulate along the K dimension.\n",
    "        accumulator = tl.dot(a, b, accumulator)\n",
    "        # Advance the ptrs to the next K block.\n",
    "        a_ptrs += BLOCK_SIZE_K * stride_ak\n",
    "        b_ptrs += BLOCK_SIZE_K * stride_bk\n",
    "    # You can fuse arbitrary activation functions here\n",
    "    # while the accumulator is still in FP32!\n",
    "    c = accumulator.to(tl.float16)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Write back the block of the output matrix C with masks.\n",
    "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n",
    "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n",
    "    tl.store(c_ptrs, c, mask=c_mask)\n",
    "\n",
    "\n",
    "# We can fuse `leaky_relu` by providing it as an `ACTIVATION` meta-parameter in `_matmul`.\n",
    "@triton.jit\n",
    "def leaky_relu(x):\n",
    "    x = x + 1\n",
    "    return tl.where(x >= 0, x, 0.01 * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul(a, b):\n",
    "    # Check constraints.\n",
    "    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n",
    "    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    M, K = a.shape\n",
    "    K, N = b.shape\n",
    "    # Allocates output.\n",
    "    c = torch.empty((M, N), device=a.device, dtype=torch.float16)\n",
    "    # 1D launch kernel where each block gets its own program.\n",
    "    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), )\n",
    "    matmul_kernel[grid](\n",
    "        a, b, c,  #\n",
    "        M, N, K,  #\n",
    "        a.stride(0), a.stride(1),  #\n",
    "        b.stride(0), b.stride(1),  #\n",
    "        c.stride(0), c.stride(1),  #\n",
    "    )\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_matmul(a, b):\n",
    "    # Check constraints.\n",
    "    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n",
    "    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    M, K = a.shape\n",
    "    K, N = b.shape\n",
    "    # Allocates output.\n",
    "    c = torch.empty((M, N), device=a.device, dtype=torch.float16)\n",
    "    # 1D launch kernel where each block gets its own program.\n",
    "    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), )\n",
    "    naive_matmul_kernel[grid](\n",
    "        a, b, c,  #\n",
    "        M, N, K,  #\n",
    "        a.stride(0), a.stride(1),  #\n",
    "        b.stride(0), b.stride(1),  #\n",
    "        c.stride(0), c.stride(1),  #\n",
    "    )\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "a = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n",
    "b = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n",
    "triton_output = naive_matmul(a, b)\n",
    "torch_output = torch.matmul(a, b)\n",
    "print(f\"triton_output_with_fp16_inputs={triton_output}\")\n",
    "print(f\"torch_output_with_fp16_inputs={torch_output}\")\n",
    "if torch.allclose(triton_output, torch_output, atol=1e-1, rtol=0):\n",
    "    print(\"✅ Triton and Torch match\")\n",
    "else:\n",
    "    print(\"❌ Triton and Torch differ\")\n",
    "\n",
    "TORCH_HAS_FP8 = hasattr(torch, \"float8_e5m2\")\n",
    "if TORCH_HAS_FP8:\n",
    "    torch.manual_seed(0)\n",
    "    a = torch.randn((512, 512), device=\"cuda\", dtype=torch.float16)\n",
    "    b = torch.randn((512, 512), device=\"cuda\", dtype=torch.float16)\n",
    "    a = a.to(torch.float8_e5m2)\n",
    "    # pre-transpose b for efficiency.\n",
    "    b = b.T\n",
    "    b = b.to(torch.float8_e5m2)\n",
    "    triton_output = matmul(a, b)\n",
    "    torch_output = torch.matmul(a.to(torch.float16), b.to(torch.float16))\n",
    "    print(f\"triton_output_with_fp8_inputs={triton_output}\")\n",
    "    print(f\"torch_output_with_fp8_inputs={torch_output}\")\n",
    "    if torch.allclose(triton_output, torch_output, atol=0.125, rtol=0):\n",
    "        print(\"✅ Triton and Torch match\")\n",
    "    else:\n",
    "        print(\"❌ Triton and Torch differ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([], device='cuda:0', dtype=torch.int64),)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(triton_output[0]==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-7.2266e+00, -1.1156e+01, -2.4750e+01, -3.4625e+01, -4.7383e+00,\n",
       "         1.8344e+01,  2.4688e+01,  5.3086e+00, -5.0117e+00, -1.8391e+01,\n",
       "         5.8781e+01,  1.7250e+01, -7.4453e+00, -2.5422e+01, -9.7188e+00,\n",
       "        -1.2734e+01,  1.6797e+01,  4.4219e+01,  1.0516e+01, -1.0352e+01,\n",
       "        -1.7297e+01,  3.7832e+00, -1.9672e+01, -2.3496e+00, -8.2969e+00,\n",
       "         2.3000e+01,  2.8141e+01, -2.5047e+01, -4.1188e+01, -4.2312e+01,\n",
       "         6.4727e+00, -8.4375e+00,  2.0984e+01, -1.1500e+01,  6.8359e+00,\n",
       "         1.9000e+01,  9.9609e+00, -7.6953e-01, -3.2480e+00,  2.4484e+01,\n",
       "         8.4453e+00, -1.7047e+01, -7.8672e+00, -2.2297e+01,  1.0891e+01,\n",
       "        -1.9312e+01, -5.7438e+01,  8.1328e+00,  3.1125e+01, -1.2820e+01,\n",
       "        -7.8281e+00, -4.7031e+01,  1.0789e+01, -3.3500e+01, -1.5867e+01,\n",
       "        -3.7617e+00,  2.1219e+01,  1.9703e+01, -5.7734e+00, -2.3656e+01,\n",
       "         4.9141e+00, -1.1650e+00, -9.7578e+00,  3.5969e+01,  1.5875e+01,\n",
       "        -2.0562e+01,  6.5039e+00,  2.1035e+00, -1.4398e+01, -3.6469e+01,\n",
       "        -1.8469e+01,  3.5156e+01,  1.3578e+01, -7.6562e+00,  1.1586e+01,\n",
       "         2.0391e+01,  3.5410e+00, -8.7969e+00, -5.0719e+01,  2.0594e+01,\n",
       "         1.2373e+00, -1.5484e+01,  1.2844e+01,  1.1133e+01,  1.8219e+01,\n",
       "         4.9297e+00, -2.7250e+01,  1.2211e+01,  2.0195e+00,  1.3516e+01,\n",
       "        -2.6289e+00,  1.9438e+01,  7.0859e+00, -1.0969e+01,  3.8625e+01,\n",
       "        -7.5039e+00,  1.4859e+01,  5.7109e+00, -7.9219e+00,  1.6359e+01,\n",
       "         2.9281e+01, -7.2875e+01, -2.4023e+00,  1.4570e+01, -1.1844e+01,\n",
       "         3.5098e+00,  4.2781e+01,  6.8506e-01, -1.4586e+01,  7.9688e+00,\n",
       "        -1.9672e+01, -7.2773e+00,  1.3625e+01, -1.0773e+01, -2.0750e+01,\n",
       "         1.1609e+01,  1.7828e+01, -2.1234e+01,  2.8453e+01,  9.3047e+00,\n",
       "         9.9141e+00, -2.6000e+01, -1.7625e+01,  1.3500e+01,  2.5176e+00,\n",
       "        -1.4023e+01,  4.0656e+01,  1.3242e+01, -1.0016e+01,  1.9922e+01,\n",
       "         5.5062e+01, -2.2219e+01, -1.2102e+01,  1.7844e+01, -1.9641e+01,\n",
       "        -3.7281e+01,  7.8945e+00, -1.6719e+01,  2.2375e+01, -3.4344e+01,\n",
       "         2.2641e+01, -1.5156e+01, -2.8938e+01, -3.0859e+01,  5.1250e+00,\n",
       "        -2.3828e+01,  8.1641e+00,  1.0164e+01, -1.1906e+01,  2.6602e+00,\n",
       "         3.5371e+00,  4.1281e+01, -6.3031e+01,  4.4648e+00, -3.8250e+01,\n",
       "         1.0430e+00,  2.2594e+01, -4.1235e-01,  5.3398e+00,  3.1500e+01,\n",
       "        -6.2617e+00,  1.8062e+01,  1.0602e+01,  4.7734e+00, -3.6844e+01,\n",
       "         1.4727e+01, -1.7016e+01,  3.1156e+01,  2.7234e+01, -4.7969e+01,\n",
       "        -1.6297e+01,  3.9656e+01,  2.1344e+01, -1.4008e+01,  5.7070e+00,\n",
       "         3.8969e+01, -1.2727e+01,  1.8359e+01, -4.9750e+01,  7.9648e+00,\n",
       "        -9.5391e+00,  3.8969e+01,  1.9561e+00,  1.4492e+00, -1.4328e+01,\n",
       "        -3.0078e+00, -5.5625e+00, -1.5469e+01, -2.4391e+01, -3.0078e+01,\n",
       "        -5.5156e+00, -1.7656e+01,  7.8711e+00, -3.9922e+00, -4.9531e+00,\n",
       "         1.2305e+00,  1.4711e+01,  2.5957e+00,  7.8594e+00,  1.3680e+01,\n",
       "         3.5840e+00,  1.7236e+00, -1.7078e+01, -9.6641e+00, -2.1141e+01,\n",
       "        -1.6781e+01, -5.8008e+00,  4.4219e+01,  4.2603e-01,  3.5219e+01,\n",
       "         1.4836e+01,  3.7344e+01, -3.0922e+01,  4.2438e+01,  2.1078e+01,\n",
       "        -9.8672e+00,  3.4844e+01,  2.0438e+01, -1.1172e+01,  1.3930e+01,\n",
       "         1.1475e+00,  1.3916e+00,  5.0312e+01,  8.3301e-01,  1.3484e+01,\n",
       "         5.8438e+00,  2.6133e+00, -8.6562e+00, -1.3105e+00, -9.5156e+00,\n",
       "        -3.1656e+01, -7.9414e+00, -3.2625e+01, -2.5312e+01,  4.2656e+00,\n",
       "        -7.1924e-01,  2.8844e+01, -5.0195e+00, -4.1688e+01, -4.2531e+01,\n",
       "        -2.5109e+01,  1.3531e+01,  2.4094e+01,  4.1250e+01,  3.7109e+00,\n",
       "         7.9062e+00,  1.1258e+01, -9.9609e+00, -1.8922e+01, -8.4688e+00,\n",
       "        -3.3500e+01, -5.4969e+01,  2.5898e+00, -1.0156e+01,  9.9297e+00,\n",
       "         4.5406e+01, -2.3145e+00, -1.9199e+00, -1.1102e+01,  3.2625e+01,\n",
       "        -2.0203e+01,  7.4648e+00,  2.8266e+01, -6.3906e+00,  4.1125e+01,\n",
       "        -8.2578e+00, -1.5414e+01, -3.1641e+00,  2.7500e+00, -5.2969e+00,\n",
       "         9.3281e+00, -5.3750e+00,  1.2016e+01, -1.0078e+00, -3.3356e-02,\n",
       "        -1.2492e+01,  4.1938e+01, -1.7517e-01,  2.4875e+01, -1.5492e+01,\n",
       "         3.5219e+01,  3.9629e+00, -9.3672e+00, -7.9688e+00, -4.3711e+00,\n",
       "        -5.9875e+01, -1.9609e+01,  1.9391e+01,  1.9422e+01,  3.5906e+01,\n",
       "         1.8916e+00,  2.7227e+00,  8.8359e+00,  3.1787e-01, -2.1375e+01,\n",
       "        -5.3242e+00,  7.8867e+00,  4.1992e+00,  2.3828e+00,  4.2969e+00,\n",
       "        -3.3875e+01, -3.4688e+01, -1.5010e+00, -3.0500e+01, -1.0398e+01,\n",
       "        -3.7781e+01, -9.2656e+00,  1.6453e+01,  3.9281e+01,  2.0859e+01,\n",
       "        -2.7891e+01,  2.1035e+00, -9.3516e+00,  4.1844e+01, -1.1961e+01,\n",
       "         2.5547e+00, -8.9219e+00,  2.0500e+01,  1.0133e+01,  5.6992e+00,\n",
       "        -1.5078e+01,  1.6602e+00, -1.9359e+01, -1.5336e+01,  1.9541e+00,\n",
       "        -3.2688e+01, -3.6963e-01,  2.3984e+01,  4.2125e+01,  4.3828e+00,\n",
       "         3.2531e+01, -1.8594e+01, -1.1727e+01,  4.5508e+00, -2.0891e+01,\n",
       "        -3.4344e+01,  1.4672e+01,  2.6781e+01, -2.7168e+00, -4.9609e+00,\n",
       "         2.6422e+01,  2.0469e+00, -6.2148e+00,  4.5531e+01, -1.7781e+01,\n",
       "        -2.3875e+01, -1.8828e+01,  3.4094e+01,  8.5645e-01, -2.2125e+01,\n",
       "        -1.8094e+01, -2.6270e+00, -7.8672e+00, -1.5633e+01,  2.4094e+01,\n",
       "         2.0781e+01, -1.0266e+01,  6.5137e-01, -1.8672e+01,  4.4375e+01,\n",
       "        -3.5156e+01,  4.0352e+00,  5.0469e+00, -5.0859e+00,  8.5312e+00,\n",
       "         9.8203e+00, -9.8906e+00, -1.9203e+01,  2.6031e+01,  7.6953e+00,\n",
       "        -1.7672e+01, -3.9000e+01, -1.1219e+01,  7.6680e+00,  3.7031e+01,\n",
       "         1.4781e+01, -1.5602e+01, -2.1953e+01, -1.8094e+01,  2.9797e+01,\n",
       "        -1.0078e+01, -4.7469e+01, -1.6162e+00, -4.9219e+00, -2.0688e+01,\n",
       "        -5.0859e+00, -1.4133e+01,  3.2312e+01, -1.3586e+01, -3.0812e+01,\n",
       "        -4.9973e-03,  1.9078e+01, -3.2062e+01,  1.2949e+00, -1.0375e+01,\n",
       "         1.2766e+01,  3.7562e+01, -2.0406e+01, -5.1641e+00,  2.2453e+01,\n",
       "        -8.4453e+00,  4.4125e+01, -8.2109e+00,  1.7781e+01,  2.5047e+01,\n",
       "         9.5859e+00,  3.1250e+00,  1.3875e+01, -1.8203e+01,  7.9199e-01,\n",
       "         3.7375e+01,  2.0984e+01,  1.8453e+01, -1.0094e+01,  1.1539e+01,\n",
       "        -1.4756e+00, -8.5156e+00,  3.9688e+00,  1.6688e+01, -8.0391e+00,\n",
       "        -5.9297e+00, -1.7797e+01, -6.6094e+00, -3.0703e+00,  3.0391e+01,\n",
       "        -4.8719e+01,  2.8125e+01,  5.7227e+00, -8.0625e+00,  1.6453e+01,\n",
       "        -5.1445e+00,  9.5547e+00, -2.1719e+01,  1.0967e+00,  1.6943e+00,\n",
       "        -4.1719e+00, -1.2305e+01,  8.3672e+00,  1.8531e+01, -1.1891e+01,\n",
       "         6.6289e+00,  1.1711e+01,  5.1221e-01, -2.5312e+01, -4.3750e+00,\n",
       "        -1.3438e+00,  2.0781e+01,  7.3750e+00, -1.9234e+01, -5.6602e+00,\n",
       "        -1.0028e-01, -1.2607e+00, -3.7062e+01, -4.0594e+01, -2.9281e+01,\n",
       "         3.9844e+00, -6.5078e+00, -2.2578e+01,  1.2125e+01, -3.1234e+01,\n",
       "        -2.4156e+01, -2.6688e+01, -4.1719e+00, -2.4547e+01,  2.9594e+01,\n",
       "        -2.0891e+01,  1.3773e+01, -3.9812e+01,  4.0000e+00,  7.3389e-01,\n",
       "         4.1895e-01,  7.5547e+00, -1.1352e+01,  4.8531e+01,  1.3031e+01,\n",
       "         4.5562e+01,  6.6016e+00,  1.3328e+01, -2.3562e+01, -6.6250e+01,\n",
       "        -1.3039e+01,  2.0449e+00,  6.9531e+00,  6.6602e+00,  4.3656e+01,\n",
       "         2.6875e+01, -5.3867e+00,  2.1602e+00,  6.8906e+00,  8.2861e-01,\n",
       "         1.1922e+01, -4.4971e-01,  8.2188e+00, -1.8328e+01, -8.8574e-01,\n",
       "         1.4672e+01, -4.9258e+00, -3.3613e+00,  3.1680e+00, -2.7070e+00,\n",
       "        -1.6750e+01, -1.7594e+01, -1.1508e+01, -1.1555e+01,  1.3195e+01,\n",
       "         2.7516e+01, -1.1922e+01,  3.2500e+01,  4.3984e+00, -1.5375e+01,\n",
       "         5.6875e+00, -2.9688e+00], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "triton",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
